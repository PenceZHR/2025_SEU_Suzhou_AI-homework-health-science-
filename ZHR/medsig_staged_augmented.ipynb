{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6018ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import timm\n",
    "import shutil\n",
    "\n",
    "# ========== 1. 设备 ==========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ========== 2. 数据路径 ==========\n",
    "# ❗❗❗ 把这个改成你“四个类别”所在的文件夹 ❗❗❗\n",
    "root_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/dataset2/\"\n",
    "\n",
    "# 目录结构要求：\n",
    "# root_dir/\n",
    "#   classA/\n",
    "#   classB/\n",
    "#   classC/\n",
    "#   classD/\n",
    "\n",
    "# ========== 3. 训练超参数 ==========\n",
    "batch_size   = 32\n",
    "num_workers  = 0\n",
    "num_epochs   = 30\n",
    "lr           = 1e-3      # 只训练线性头，可以稍微大一点\n",
    "weight_decay = 1e-2\n",
    "\n",
    "# ========== 4. 随机种子（保证每次划分一致） ==========\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "local_model_dir = r\"/root/人工智能基础期末作业/medsig_model\"  # TODO: 改成你的路径\n",
    "\n",
    "train_dir = r\"/root/人工智能基础期末作业/data_split/train\"\n",
    "val_dir = r\"/root/人工智能基础期末作业/data_split/val\"\n",
    "image_size = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30cb06ff-3143-43b0-9ed8-222b284fb2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class 0', 'class 1', 'class 2', 'class 3', '.DS_Store']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DIR = Path(\"/root/人工智能基础期末作业/dataset2/\")\n",
    "OUT_DIR = Path(\"/root/人工智能基础期末作业/data_split\")    # 拆分后的 train/val 会放在这里\n",
    "\n",
    "classes = os.listdir(RAW_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fffe203e-3940-4937-b990-d247cfbc58b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现的类别： ['class 0', 'class 1', 'class 2', 'class 3']\n"
     ]
    }
   ],
   "source": [
    "classes = [d for d in os.listdir(RAW_DIR) if (RAW_DIR / d).is_dir()]\n",
    "print(\"发现的类别：\", classes)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for cls in classes:\n",
    "        (OUT_DIR / phase / cls).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75718a6b-6cdd-4579-ac19-12efac4f9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.8   # 训练集占 80%\n",
    "# val_ratio   = 0.2   # 验证集占 20%，train_ratio + val_ratio 应该 = 1\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"使用设备:\", device)\n",
    "\n",
    "\n",
    "# for cls in classes:\n",
    "#     src_dir = RAW_DIR / cls\n",
    "#     files = [f for f in os.listdir(src_dir)\n",
    "#              if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "#     random.shuffle(files)\n",
    "#     n = len(files)\n",
    "#     n_train = int(n * train_ratio)\n",
    "#     # val 集就是剩下的\n",
    "#     train_files = files[:n_train]\n",
    "#     val_files   = files[n_train:]\n",
    "\n",
    "#     print(f\"{cls}: 总数 {n}, 训练 {len(train_files)}, 验证 {len(val_files)}\")\n",
    "\n",
    "#     # 复制到目标文件夹（想省空间可以用 shutil.move）\n",
    "#     for fname in train_files:\n",
    "#         shutil.copy(src_dir / fname, OUT_DIR / \"train\" / cls / fname)\n",
    "#     for fname in val_files:\n",
    "#         shutil.copy(src_dir / fname, OUT_DIR / \"val\" / cls / fname)\n",
    "\n",
    "# print(\"划分完成，已保存到\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e04495e-54e2-48f9-9e89-14281df21e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from local dir: /root/人工智能基础期末作业/medsig_model\n",
      "raw_model class: <class 'transformers.models.siglip.modeling_siglip.SiglipModel'>\n",
      "Use raw_model.vision_model as image encoder.\n",
      "image embed dim: 1152\n",
      "使用 2 张 GPU 进行 DataParallel\n",
      "总参数量: 428,570,052\n",
      "当前可训练参数量(仅 head): 4,612\n"
     ]
    }
   ],
   "source": [
    "# %% 从本地加载 MedSigLIP，多模态模型中抽出视觉塔\n",
    "print(\"Loading base model from local dir:\", local_model_dir)\n",
    "\n",
    "raw_model = AutoModel.from_pretrained(\n",
    "    local_model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "print(\"raw_model class:\", type(raw_model))\n",
    "\n",
    "# ---- 关键：只抽出“视觉 encoder” ----\n",
    "if hasattr(raw_model, \"vision_model\"):\n",
    "    # 大多数 CLIP/SigLIP 多模态模型的视觉塔都叫 vision_model\n",
    "    img_encoder = raw_model.vision_model\n",
    "    print(\"Use raw_model.vision_model as image encoder.\")\n",
    "elif hasattr(raw_model, \"get_image_features\"):\n",
    "    # 有些实现是直接在 model 上提供 get_image_features\n",
    "    img_encoder = raw_model\n",
    "    print(\"Use raw_model itself as image encoder (get_image_features).\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"在 raw_model 里找不到 vision_model 或 get_image_features，\"\n",
    "        \"请 print(raw_model) 看看结构，然后再定位视觉塔。\"\n",
    "    )\n",
    "\n",
    "img_encoder.to(device)\n",
    "img_encoder.eval()\n",
    "\n",
    "# ---- 用 dummy 探测 embedding 维度 ----\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 3, image_size, image_size).to(device)   # [1,3,448,448]\n",
    "\n",
    "    try:\n",
    "        out = img_encoder(pixel_values=dummy)   # 优先用 keyword\n",
    "    except TypeError:\n",
    "        out = img_encoder(dummy)               # 有的模型只收 positional\n",
    "\n",
    "    if hasattr(out, \"image_embeds\"):\n",
    "        feats = out.image_embeds                    # [1, D]\n",
    "    elif hasattr(out, \"pooler_output\"):\n",
    "        feats = out.pooler_output                   # [1, D]\n",
    "    elif hasattr(out, \"last_hidden_state\"):\n",
    "        feats = out.last_hidden_state.mean(dim=1)   # [1, D]\n",
    "    elif isinstance(out, torch.Tensor):\n",
    "        feats = out\n",
    "    else:\n",
    "        print(\"Unknown output type:\", type(out))\n",
    "        print(out)\n",
    "        raise RuntimeError(\n",
    "            \"无法从 img_encoder 的输出中找到特征，请 print(out) 再调整逻辑。\"\n",
    "        )\n",
    "\n",
    "embed_dim = feats.shape[-1]\n",
    "print(\"image embed dim:\", embed_dim)\n",
    "\n",
    "\n",
    "# ---- 封装分类模型：视觉塔 + 线性 head ----\n",
    "class MedSigVisionClassifier(nn.Module):\n",
    "    def __init__(self, img_encoder, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = img_encoder\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # pixel_values: [B,3,448,448]\n",
    "        try:\n",
    "            out = self.encoder(pixel_values=pixel_values)\n",
    "        except TypeError:\n",
    "            out = self.encoder(pixel_values)\n",
    "\n",
    "        if hasattr(out, \"image_embeds\"):\n",
    "            feats = out.image_embeds\n",
    "        elif hasattr(out, \"pooler_output\"):\n",
    "            feats = out.pooler_output\n",
    "        elif hasattr(out, \"last_hidden_state\"):\n",
    "            feats = out.last_hidden_state.mean(dim=1)\n",
    "        elif isinstance(out, torch.Tensor):\n",
    "            feats = out\n",
    "        else:\n",
    "            raise RuntimeError(\"encoder 输出里找不到特征，需根据实际结构单独处理。\")\n",
    "\n",
    "        # L2 归一化（保持和 CLIP 系一致的风格）\n",
    "        feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        logits = self.head(feats)    # [B,num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ==== 构建模型 + DataParallel ====\n",
    "model = MedSigVisionClassifier(img_encoder, embed_dim, 4)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"使用\", torch.cuda.device_count(), \"张 GPU 进行 DataParallel\")\n",
    "    model = nn.DataParallel(model)   # 在多卡上自动切 batch\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 关键：取出真正的模型（DataParallel 包了一层壳）\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# 先冻结视觉塔，只训 head 当 baseline\n",
    "for p in core.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in core.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# 统计参数量用 core（真正的模型）\n",
    "total_params = sum(p.numel() for p in core.parameters())\n",
    "trainable_params = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "print(f\"总参数量: {total_params:,}\")\n",
    "print(f\"当前可训练参数量(仅 head): {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2319cc0e-b2e4-4da0-9aae-f1f826a06f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: cannot load processor mean/std, fallback to 0.5. err = \n",
      "SiglipTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "classes: ['class 0', 'class 1', 'class 2', 'class 3']\n",
      "train size: 5841 val size: 1462\n"
     ]
    }
   ],
   "source": [
    "# %% DataLoader：胸片增强版（更稳、更贴近 X-ray）\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------- 1) 尽量用 MedSig 的 processor 里的 mean/std（更匹配预训练） ----------\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(local_model_dir, local_files_only=True)\n",
    "    img_proc = getattr(processor, \"image_processor\", processor)  # 兼容不同 processor 结构\n",
    "    mean = list(getattr(img_proc, \"image_mean\", [0.5, 0.5, 0.5]))\n",
    "    std  = list(getattr(img_proc, \"image_std\",  [0.5, 0.5, 0.5]))\n",
    "    print(\"Use processor mean/std:\", mean, std)\n",
    "except Exception as e:\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std  = [0.5, 0.5, 0.5]\n",
    "    print(\"WARN: cannot load processor mean/std, fallback to 0.5. err =\", e)\n",
    "\n",
    "# ---------- 2) 胸片增强：轻几何 + 轻强度 + 少量噪声/模糊 ----------\n",
    "# ⚠️ 胸片很多任务不建议默认左右翻转（会改变左右肺解剖信息）\n",
    "use_hflip = False  # 若你确定标签与左右无关（例如“是否肺炎”），可改 True\n",
    "\n",
    "resize_pad = int(image_size * 1.10)  # 先略放大再裁回，避免硬拉伸\n",
    "\n",
    "train_tfms_list = [\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(resize_pad, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "]\n",
    "\n",
    "if use_hflip:\n",
    "    train_tfms_list.append(transforms.RandomHorizontalFlip(p=0.5))\n",
    "\n",
    "train_tfms_list += [\n",
    "    transforms.RandomAffine(\n",
    "        degrees=5,                 # 小角度旋转\n",
    "        translate=(0.03, 0.03),    # 小平移\n",
    "        scale=(0.95, 1.05),        # 小缩放\n",
    "        interpolation=transforms.InterpolationMode.BILINEAR,\n",
    "        fill=0\n",
    "    ),\n",
    "    transforms.CenterCrop(image_size),  # 用 CenterCrop 比 RandomResizedCrop 更不容易裁掉病灶\n",
    "    transforms.RandomApply(\n",
    "        [transforms.ColorJitter(brightness=0.10, contrast=0.10)],\n",
    "        p=0.8\n",
    "    ),\n",
    "    transforms.RandomAutocontrast(p=0.2),  # 轻微增强对比度（对胸片常有用）\n",
    "    transforms.RandomApply(\n",
    "        [transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 1.0))],\n",
    "        p=0.10\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomApply(\n",
    "        [transforms.Lambda(lambda x: (x + torch.randn_like(x) * 0.02).clamp(0.0, 1.0))],\n",
    "        p=0.15\n",
    "    ),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "]\n",
    "\n",
    "train_transform = transforms.Compose(train_tfms_list)\n",
    "\n",
    "# val/test：不做随机增强，只做统一预处理\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(resize_pad, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# ---------- 3) Dataset / DataLoader ----------\n",
    "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "print(\"classes:\", train_dataset.classes)\n",
    "print(\"train size:\", len(train_dataset), \"val size:\", len(val_dataset))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60070563-7824-4bec-b041-60a89183a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 blocks: encoder.layers, 数量=27\n",
      "[stage1] total=428,570,052 trainable=4,612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23603/2475904739.py:127: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e2dde96e5c49608afe0c987e3ac8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23603/2475904739.py:152: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n",
      "/tmp/ipykernel_23603/2475904739.py:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01/30] train_loss=1.3032 train_acc=0.5044 | val_loss=1.1589 val_acc=0.6737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1666e60efab4521acad04e3a0684188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02/30] train_loss=1.0089 train_acc=0.7552 | val_loss=0.8675 val_acc=0.8242\n",
      "[switch->stage2 @epoch3] total=428,570,052 trainable=30,485,924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab848da77994f34b894312d69fc2ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03/30] train_loss=0.7007 train_acc=0.8422 | val_loss=0.6046 val_acc=0.8721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6a7bde5b7041f3a482e878d42fed90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04/30] train_loss=0.5944 train_acc=0.8649 | val_loss=0.5855 val_acc=0.8741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d602597c4a248998b5fbb67c30a9fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05/30] train_loss=0.5779 train_acc=0.8749 | val_loss=0.5739 val_acc=0.8865\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b97000cdb9a4d94afb02346af9d2385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06/30] train_loss=0.5661 train_acc=0.8791 | val_loss=0.5639 val_acc=0.8912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06df865b0f1e4b199f68bd37b5bcdfc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 07/30] train_loss=0.5584 train_acc=0.8817 | val_loss=0.5632 val_acc=0.8871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0205d50ac6d74bccbd76dbed1b94eace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 8:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 08/30] train_loss=0.5511 train_acc=0.8853 | val_loss=0.5517 val_acc=0.8933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec0102f45654bae8ce936a5db586e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 9:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 09/30] train_loss=0.5441 train_acc=0.8851 | val_loss=0.5500 val_acc=0.8871\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53161faaa6ba4245a5f93eb64ad1786f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 10:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/30] train_loss=0.5420 train_acc=0.8889 | val_loss=0.5448 val_acc=0.8953\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26debbe97e7847398436350cac05ab4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 11:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/30] train_loss=0.5338 train_acc=0.8951 | val_loss=0.5451 val_acc=0.8912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c242049aff4b9f8aacc8996bfa7c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 12:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/30] train_loss=0.5315 train_acc=0.8964 | val_loss=0.5438 val_acc=0.8912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d230d9ef1d4ef786f01f58f2ec438c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 13:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/30] train_loss=0.5250 train_acc=0.9016 | val_loss=0.5378 val_acc=0.8967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d4b887d1b048bb8df58d595ae052fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 14:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/30] train_loss=0.5217 train_acc=0.8998 | val_loss=0.5386 val_acc=0.9001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652eb1a2ab3a42349222d1f26e0ab64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 15:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/30] train_loss=0.5156 train_acc=0.9082 | val_loss=0.5363 val_acc=0.8947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b931592fa1fb491aa20b67559e7ae24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 16:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/30] train_loss=0.5124 train_acc=0.9070 | val_loss=0.5345 val_acc=0.9001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30edb63151c74b44a5af78aecd1c374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 17:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/30] train_loss=0.5087 train_acc=0.9089 | val_loss=0.5335 val_acc=0.9022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd7e74a474f4e3a89e265cd751ff44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 18:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/30] train_loss=0.5087 train_acc=0.9087 | val_loss=0.5343 val_acc=0.9063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9087240853e34964a35c74c3773314bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 19:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/30] train_loss=0.5021 train_acc=0.9130 | val_loss=0.5348 val_acc=0.9008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1e11d3f768400abc0b75bba029d3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 20:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/30] train_loss=0.5023 train_acc=0.9130 | val_loss=0.5384 val_acc=0.8995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2622fe8032436ca2ed3389c3a68c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 21:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/30] train_loss=0.5012 train_acc=0.9158 | val_loss=0.5378 val_acc=0.9008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277022735c53424a8fef9578f9a73d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 22:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/30] train_loss=0.4986 train_acc=0.9146 | val_loss=0.5344 val_acc=0.9022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1643f6c44e4dedb119c4b1895af8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 23:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/30] train_loss=0.4987 train_acc=0.9188 | val_loss=0.5343 val_acc=0.9042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932b8eb6b3824b8e9422f30295896bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 24:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/30] train_loss=0.5000 train_acc=0.9175 | val_loss=0.5342 val_acc=0.9036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3432a52978540e4822c44296635ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 25:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/30] train_loss=0.4972 train_acc=0.9163 | val_loss=0.5348 val_acc=0.9036\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a62f0d845d4f10b054dd4932f1dc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 26:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/30] train_loss=0.4922 train_acc=0.9195 | val_loss=0.5348 val_acc=0.9015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4e9cc4d4f24a8a93107ca3b932be8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 27:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/30] train_loss=0.4961 train_acc=0.9192 | val_loss=0.5349 val_acc=0.9022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d1b042f12a4aba98c1166ce918bcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 28:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/30] train_loss=0.4968 train_acc=0.9166 | val_loss=0.5351 val_acc=0.9029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33190b08e424e79bae8f80469140b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 29:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/30] train_loss=0.4941 train_acc=0.9171 | val_loss=0.5349 val_acc=0.9029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d86a05207b4567910e235bc3887010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train epoch 30:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/30] train_loss=0.4958 train_acc=0.9161 | val_loss=0.5349 val_acc=0.9022\n",
      "Best val_acc: 0.9062927496580028\n",
      "Saved best model to: ./medsig_staged_best_acc0.9063.pth\n"
     ]
    }
   ],
   "source": [
    "# %% 分阶段解冻 + 分层学习率 + warmup cosine（MedSigLIP 推荐）\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# 处理 DataParallel 外壳\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# ====== 训练配置（你可以先按这个跑）======\n",
    "stage1_epochs = 2                 # 只训 head\n",
    "stage2_epochs = 8                 # 解冻最后 1/3 blocks\n",
    "label_smoothing = 0.10\n",
    "head_lr = 1e-3\n",
    "backbone_lr = 2e-5               # 建议 1e-5 ~ 3e-5\n",
    "grad_clip = 1.0\n",
    "use_amp = (device == \"cuda\")\n",
    "\n",
    "assert num_epochs >= stage1_epochs + stage2_epochs, \"num_epochs 太小，至少要 >= stage1+stage2\"\n",
    "\n",
    "# ====== 找到 ViT blocks（尽量兼容不同结构）======\n",
    "def _get_by_path(obj, path: str):\n",
    "    cur = obj\n",
    "    for name in path.split(\".\"):\n",
    "        if not hasattr(cur, name):\n",
    "            return None\n",
    "        cur = getattr(cur, name)\n",
    "    return cur\n",
    "\n",
    "def get_blocks(encoder):\n",
    "    candidates = [\n",
    "        \"encoder.layers\",                 # e.g., CLIPVisionModel.encoder.layers\n",
    "        \"encoder.encoder.layers\",         # 你 notebook 里原来用的\n",
    "        \"vision_model.encoder.layers\",    # 少数实现\n",
    "        \"transformer.layers\",\n",
    "        \"layers\",\n",
    "        \"blocks\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        blk = _get_by_path(encoder, p)\n",
    "        if blk is not None and hasattr(blk, \"__len__\"):\n",
    "            return blk, p\n",
    "    raise RuntimeError(\"找不到 encoder 的 block 列表（layers/blocks）。你可以 print(core.encoder) 看结构再补路径。\")\n",
    "\n",
    "blocks, blk_path = get_blocks(core.encoder)\n",
    "n_blocks = len(blocks)\n",
    "print(f\"找到 blocks: {blk_path}, 数量={n_blocks}\")\n",
    "\n",
    "def set_trainable(stage: int):\n",
    "    '''\n",
    "    stage=1: 冻结 backbone，只训 head\n",
    "    stage=2: 解冻最后 1/3 blocks + head\n",
    "    stage=3: 解冻最后 2/3 blocks + head（更强但更易过拟合）\n",
    "    '''\n",
    "    # 1) 全冻结\n",
    "    for p in core.encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    # 2) head 永远训\n",
    "    for p in core.head.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    if stage == 1:\n",
    "        return\n",
    "\n",
    "    if stage == 2:\n",
    "        start = int(math.floor(n_blocks * (8/9)))+1  # 最后 1/3\n",
    "    elif stage == 3:\n",
    "        start = int(math.floor(n_blocks * (1/9)))-1  # 最后 2/3\n",
    "    else:\n",
    "        raise ValueError(\"stage must be 1/2/3\")\n",
    "\n",
    "    # 3) 解冻 blocks[start:]\n",
    "    for b in blocks[start:]:\n",
    "        for p in b.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # 4) 常见：最后的 LayerNorm / post_layernorm 也一并解冻（有则解）\n",
    "    for ln_name in [\"post_layernorm\", \"post_layer_norm\", \"layernorm\", \"layer_norm\", \"ln_post\", \"final_layer_norm\", \"norm\"]:\n",
    "        m = getattr(core.encoder, ln_name, None)\n",
    "        if m is not None:\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "def count_trainable():\n",
    "    tot = sum(p.numel() for p in core.parameters())\n",
    "    tr  = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "    return tot, tr\n",
    "\n",
    "# 初始 stage1\n",
    "set_trainable(stage=1)\n",
    "tot, tr = count_trainable()\n",
    "print(f\"[stage1] total={tot:,} trainable={tr:,}\")\n",
    "\n",
    "# ====== optimizer：分层学习率（head/backbone）======\n",
    "optimizer = AdamW(\n",
    "    [\n",
    "        {\"params\": core.head.parameters(), \"lr\": head_lr, \"weight_decay\": weight_decay},\n",
    "        {\"params\": core.encoder.parameters(), \"lr\": backbone_lr, \"weight_decay\": weight_decay},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ====== scheduler：warmup + cosine（按 step 调度）======\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "warmup_steps = max(1, int(0.05 * total_steps))\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "# label smoothing（PyTorch 老版本可能不支持 label_smoothing 参数，做个兼容）\n",
    "try:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "except TypeError:\n",
    "    def _ls_ce(logits, target, eps: float):\n",
    "        n_class = logits.size(-1)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        # 标准 CE\n",
    "        nll = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        # 平滑项：均匀分布\n",
    "        smooth = -log_probs.mean(dim=-1)\n",
    "        return ((1 - eps) * nll + eps * smooth).mean()\n",
    "    criterion = lambda logits, labels: _ls_ce(logits, labels, label_smoothing)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch():\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / max(total, 1), correct / max(total, 1)\n",
    "\n",
    "def train_one_epoch(epoch: int, global_step: int):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Train epoch {epoch}\", leave=False)\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if grad_clip is not None and grad_clip > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{running_loss/max(total,1):.4f}\", acc=f\"{correct/max(total,1):.4f}\",\n",
    "                         lr_head=f\"{optimizer.param_groups[0]['lr']:.2e}\",\n",
    "                         lr_backbone=f\"{optimizer.param_groups[1]['lr']:.2e}\")\n",
    "\n",
    "    return running_loss / max(total, 1), correct / max(total, 1), global_step\n",
    "\n",
    "# ====== 主训练循环（带分阶段解冻）======\n",
    "torch.cuda.empty_cache()\n",
    "log_path = \"trainlog_staged_unfreeze.log\"\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # stage 切换点：epoch 开始前切\n",
    "    if epoch == stage1_epochs + 1:\n",
    "        set_trainable(stage=2)\n",
    "        tot, tr = count_trainable()\n",
    "        print(f\"[switch->stage2 @epoch{epoch}] total={tot:,} trainable={tr:,}\")\n",
    "    # if epoch == stage1_epochs + stage2_epochs + 1:\n",
    "    #     set_trainable(stage=3)\n",
    "    #     tot, tr = count_trainable()\n",
    "    #     print(f\"[switch->stage3 @epoch{epoch}] total={tot:,} trainable={tr:,}\")\n",
    "\n",
    "    tr_loss, tr_acc, global_step = train_one_epoch(epoch, global_step)\n",
    "    va_loss, va_acc = eval_one_epoch()\n",
    "\n",
    "    line = (f\"[Epoch {epoch:02d}/{num_epochs}] \"\n",
    "            f\"train_loss={tr_loss:.4f} train_acc={tr_acc:.4f} | \"\n",
    "            f\"val_loss={va_loss:.4f} val_acc={va_acc:.4f}\")\n",
    "    print(line)\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        best_state_dict = (model.module if isinstance(model, nn.DataParallel) else model).state_dict()\n",
    "\n",
    "print(\"Best val_acc:\", best_val_acc)\n",
    "if best_state_dict is not None:\n",
    "    save_path = f\"./medsig_staged_best_acc{best_val_acc:.4f}.pth\"\n",
    "    torch.save(best_state_dict, save_path)\n",
    "    print(\"Saved best model to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d81cc4a-a938-4212-9c81-74a07abce59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell7 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93801fda-6f42-4015-a3c4-16edeab52edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell8 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75341aa5-acc9-44bc-9313-6aba8c54ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell9 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71358ddc-31c0-4705-bdaf-fc253f6afba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell10 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ac692-703d-4756-9db7-2a178966dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell11 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2825ec-efd3-4c58-968f-4db3960a2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# （已替换）原 cell12 的训练/保存逻辑已被 '分阶段解冻' cell 覆盖。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b334dcf-3191-4017-9bbf-694964113720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 先构建“同结构”的模型 ------------------------------------\n",
    "# 这里用你自己的模型类和参数\n",
    "# 比如你的是：\n",
    "# model_core = MedSigVisionClassifier(img_encoder, embed_dim, num_classes)\n",
    "torch.cuda.empty_cache()\n",
    "model_core = MedSigVisionClassifier(img_encoder, embed_dim, 4)  # 按你之前的一样写\n",
    "model_core = model_core.to(device)\n",
    "\n",
    "# 2. 读取 pth（state_dict） ------------------------------------\n",
    "state_dict_path = \"medsiglip448_cls_best_acc0.9770.pth\"   # 改成你自己的文件名\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "\n",
    "# 3. 把参数加载进模型 ------------------------------------------\n",
    "model_core.load_state_dict(state_dict)\n",
    "\n",
    "# 4. 如果你想多卡跑，可以再包一层 DataParallel（可选）\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model_core)\n",
    "else:\n",
    "    model = model_core\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ 模型加载完毕，可以直接用 model 做推理/验证\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff7b15-d680-4108-be77-a90ee6b03d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_once(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(\"acc =\", correct / total)\n",
    "\n",
    "# 比如在 val_loader / test_loader 上跑一下\n",
    "eval_once(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a84e7-3336-47d9-a57a-4d9df87185ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
