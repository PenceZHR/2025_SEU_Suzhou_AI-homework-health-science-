{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13651e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"使用设备:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20afa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'baseline_Conv_transformer.ipynb',\n",
       " 'baseline_resnet.ipynb',\n",
       " 'BiomedCLIP_baseline.ipynb',\n",
       " 'class 0',\n",
       " 'class 1',\n",
       " 'class 2',\n",
       " 'class 3',\n",
       " 'CLIP ViT-L14.ipynb',\n",
       " 'medsig.ipynb',\n",
       " 'MedSigLIP.ipynb',\n",
       " 'medsiglip448_cls_best_acc0.8762.pth',\n",
       " 'medsig_lora.ipynb',\n",
       " 'trainlog_19.log',\n",
       " 'trainlog_24.log',\n",
       " 'trainlog_linear.log',\n",
       " 'trainlog_resnet.log']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DIR = Path(\"D:/OneDriveFiles/OneDrive/人工智能基础期末/dataset2\")\n",
    "OUT_DIR = Path(\"D:/OneDriveFiles/OneDrive/人工智能基础期末/data_split\")    # 拆分后的 train/val 会放在这里\n",
    "\n",
    "classes = os.listdir(RAW_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别映射： {'class 0': 0, 'class 1': 1, 'class 2': 2, 'class 3': 3}\n",
      "训练集大小： 5841\n",
      "验证集大小： 1462\n"
     ]
    }
   ],
   "source": [
    "train_tfm = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # 灰度 → 1 通道\n",
    "    transforms.Resize((448, 448)),               # 统一到 224×224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "val_tfm = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "train_set = ImageFolder(str(OUT_DIR / \"train\"), transform=train_tfm)\n",
    "val_set   = ImageFolder(str(OUT_DIR / \"val\"),   transform=val_tfm)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_set,   batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"类别映射：\", train_set.class_to_idx)\n",
    "print(\"训练集大小：\", len(train_set))\n",
    "print(\"验证集大小：\", len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8d1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        channels=(16, 16, 32, 32, 64),\n",
    "        kernel_sizes=(35, 17, 15, 9, 3, 3, 3, 3, 3),\n",
    "        strides=(1, 1, 1, 1, 1, 1, 1, 1, 1),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(channels) == 9, \"现在的设计就是 5 层 CNN，请给 5 个通道数\"\n",
    "        assert len(kernel_sizes) == 9, \"kernel_sizes 也需要是 5 个，分别对应每个卷积的 kernel_size\"\n",
    "        assert len(strides) == 9, \"strides 也需要是 5 个，分别对应每层的 stride\"\n",
    "\n",
    "        layers = []\n",
    "        c_in = in_channels\n",
    "\n",
    "        for c_out, k, s in zip(channels, kernel_sizes, strides):\n",
    "            padding = k // 2   # 保持空间尺寸不变（在 stride=1 时）\n",
    "            layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=c_in,\n",
    "                    out_channels=c_out,\n",
    "                    kernel_size=k,\n",
    "                    stride=s,\n",
    "                    padding=padding,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "            layers.append(nn.BatchNorm2d(c_out))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            # 去掉固定的 MaxPool2d，避免空间尺寸被过度缩小\n",
    "            # layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            c_in = c_out\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.out_dim = c_in\n",
    "# ...existing code...\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)                  # [B, C_last, H', W']\n",
    "        feat = self.gap(feat)                # [B, C_last, 1, 1]\n",
    "        feat = feat.view(feat.size(0), -1)   # [B, C_last]\n",
    "        return feat\n",
    "\n",
    "class ParallelCNNTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        in_channels: int = 3,  # 如果是灰度图像，这里改成 1 通道\n",
    "        branch_channels=(\n",
    "            (16,),              # 第 1 个分支，通道数为 16\n",
    "            (16,),              # 第 2 个分支\n",
    "            (16,),              # 第 3 个分支\n",
    "            (16,),              # 第 4 个分支\n",
    "            (16,),              # 第 5 个分支\n",
    "        ),\n",
    "        d_model: int = 128,    # Transformer 输入的 token 维度\n",
    "        nhead: int = 8,        # 多头注意力头数\n",
    "        num_layers: int = 4,   # Transformer 层数\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(branch_channels) == 9, \"目前就做 5 个分支，branch_channels 需要是长度为 5 的 tuple/list\"\n",
    "\n",
    "        # 1. 五个并列 CNN 分支，每个分支输出的通道数为 16\n",
    "        self.branch1 = ConvBranch(in_channels, channels=branch_channels[0])\n",
    "        self.branch2 = ConvBranch(in_channels, channels=branch_channels[1])\n",
    "        self.branch3 = ConvBranch(in_channels, channels=branch_channels[2])\n",
    "        self.branch4 = ConvBranch(in_channels, channels=branch_channels[3])\n",
    "        self.branch5 = ConvBranch(in_channels, channels=branch_channels[4])\n",
    "\n",
    "        # 确保五个分支最后的 out_dim 一样，方便后面统一投影\n",
    "        out_dims = {self.branch1.out_dim, self.branch2.out_dim, self.branch3.out_dim, self.branch4.out_dim, self.branch5.out_dim}\n",
    "        if len(out_dims) != 1:\n",
    "            raise ValueError(f\"所有分支最后输出维度不一致: {out_dims}，请把 branch_channels 配成相同最后通道数。\")\n",
    "        branch_out_dim = self.branch1.out_dim  # 比如 16\n",
    "\n",
    "        # 2. 把分支输出向量映射到 d_model 维度（token embedding）\n",
    "        self.proj = nn.Linear(branch_out_dim, d_model)\n",
    "\n",
    "        # 3. 五个 token 的可学习位置编码 (5, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 5, d_model))\n",
    "\n",
    "        # 4. Transformer Encoder：处理 [B, 5, d_model]\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,  # 输入 [B, N, d_model]\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 5. 分类头：对 5 个 token 做平均池化，再 MLP 分类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C_in, H, W]\n",
    "        # 五个分支并行处理同一张图\n",
    "        f1 = self.branch1(x)   # [B, D_branch]\n",
    "        f2 = self.branch2(x)\n",
    "        f3 = self.branch3(x)\n",
    "        f4 = self.branch4(x)\n",
    "        f5 = self.branch5(x)\n",
    "\n",
    "        # 堆成 5 个 token\n",
    "        tokens = torch.stack([f1, f2, f3, f4, f5], dim=1)  # [B, 5, D_branch]\n",
    "\n",
    "        # 线性映射到 d_model\n",
    "        x_tok = self.proj(tokens)                  # [B, 5, d_model]\n",
    "\n",
    "        # 加位置编码（区分五个分支）\n",
    "        x_tok = x_tok + self.pos_embed             # [B, 5, d_model]\n",
    "\n",
    "        # Transformer 编码\n",
    "        x_tok = self.transformer(x_tok)            # [B, 5, d_model]\n",
    "\n",
    "        # 对 5 个 token 做平均池化，得到整张图 + 五分支的综合表示\n",
    "        x_pool = x_tok.mean(dim=1)                 # [B, d_model]\n",
    "\n",
    "        # 分类\n",
    "        logits = self.classifier(x_pool)           # [B, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0639926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        in_channels: int = 3,  # 如果是灰度图像，这里改成 1 通道\n",
    "        branch_channels=(\n",
    "            (16,16,32,32,64,64,64,64,64),              # 第 1 个分支，通道数为 16\n",
    "        ),\n",
    "        d_model: int = 128,    # Transformer 输入的 token 维度\n",
    "        nhead: int = 8,        # 多头注意力头数\n",
    "        num_layers: int = 4,   # Transformer 层数\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(branch_channels) == 1, \"目前就做 5 个分支，branch_channels 需要是长度为 5 的 tuple/list\"\n",
    "\n",
    "        # 1. 五个并列 CNN 分支，每个分支输出的通道数为 16\n",
    "        self.branch1 = ConvBranch(in_channels, channels=branch_channels[0])\n",
    "\n",
    "        # 确保五个分支最后的 out_dim 一样，方便后面统一投影\n",
    "        branch_out_dim = self.branch1.out_dim  # 比如 16\n",
    "\n",
    "        # 2. 把分支输出向量映射到 d_model 维度（token embedding）\n",
    "        self.proj = nn.Linear(branch_out_dim, d_model)\n",
    "\n",
    "        # 3. 五个 token 的可学习位置编码 (5, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 5, d_model))\n",
    "\n",
    "        # 4. Transformer Encoder：处理 [B, 5, d_model]\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,  # 输入 [B, N, d_model]\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # 5. 分类头：对 5 个 token 做平均池化，再 MLP 分类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C_in, H, W]\n",
    "        # 五个分支并行处理同一张图\n",
    "        f1 = self.branch1(x)   # [B, D_branch]\n",
    "\n",
    "        # 堆成 5 个 token\n",
    "        tokens = torch.stack([f1], dim=1)  # [B, 5, D_branch]\n",
    "\n",
    "        # 线性映射到 d_model\n",
    "        x_tok = self.proj(tokens)                  # [B, 5, d_model]\n",
    "\n",
    "        # 加位置编码（区分五个分支）\n",
    "        x_tok = x_tok + self.pos_embed             # [B, 5, d_model]\n",
    "\n",
    "        # Transformer 编码\n",
    "        x_tok = self.transformer(x_tok)            # [B, 5, d_model]\n",
    "\n",
    "        # 对 5 个 token 做平均池化，得到整张图 + 五分支的综合表示\n",
    "        x_pool = x_tok.mean(dim=1)                 # [B, d_model]\n",
    "\n",
    "        # 分类\n",
    "        logits = self.classifier(x_pool)           # [B, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f65e0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNTransformer(\n",
    "    num_classes=4,   # 动态传递类别数\n",
    "    in_channels=3,             # 假设是 3 通道图像\n",
    "    branch_channels=(\n",
    "        (16,16,32,32,64,64,64,64,64,),  # 每个分支 1 层通道数\n",
    "    ),\n",
    "    d_model=128,               # Transformer 输入维度\n",
    "    nhead=8,                   # 注意力头数\n",
    "    num_layers=4,              # Transformer 层数\n",
    ").to(device)\n",
    "\n",
    "# 损失函数，交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# 优化器，Adam 优化器\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2523b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNTransformer(\n",
      "  (branch1): ConvBranch(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(3, 16, kernel_size=(35, 35), stride=(1, 1), padding=(17, 17), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 16, kernel_size=(17, 17), stride=(1, 1), padding=(8, 8), bias=False)\n",
      "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(16, 32, kernel_size=(15, 15), stride=(1, 1), padding=(7, 7), bias=False)\n",
      "      (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): Conv2d(32, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), bias=False)\n",
      "      (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (11): ReLU(inplace=True)\n",
      "      (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (14): ReLU(inplace=True)\n",
      "      (15): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (16): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (17): ReLU(inplace=True)\n",
      "      (18): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (20): ReLU(inplace=True)\n",
      "      (21): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (22): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (23): ReLU(inplace=True)\n",
      "      (24): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (25): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (26): ReLU(inplace=True)\n",
      "    )\n",
      "    (gap): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (proj): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c5c2c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    progress = tqdm(loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for x, y in progress:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_labels.extend(y.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    progress = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    for x, y in progress:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_labels.extend(y.detach().cpu().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, acc, f1, all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08cd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ec9fc849884eb48eb0dc3bf51e5c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "torch.cuda.empty_cache()\n",
    "history_hybrid = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    tr_loss, tr_acc, tr_f1 = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_one_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    history_hybrid[\"train_loss\"].append(tr_loss)\n",
    "    history_hybrid[\"val_loss\"].append(val_loss)\n",
    "    history_hybrid[\"train_acc\"].append(tr_acc)\n",
    "    history_hybrid[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch:02d}] \"\n",
    "        f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n",
    "        f\"Val loss={val_loss:.4f}, acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = model.state_dict()\n",
    "        print(\"  -> 验证集提升，更新 best 模型权重\")\n",
    "\n",
    "# 训练结束后加载 best 权重\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
