{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb541b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import timm\n",
    "import shutil\n",
    "\n",
    "# ========== 1. 设备 ==========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ========== 2. 数据路径 ==========\n",
    "# ❗❗❗ 把这个改成你“四个类别”所在的文件夹 ❗❗❗\n",
    "root_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/dataset2/\"\n",
    "\n",
    "# 目录结构要求：\n",
    "# root_dir/\n",
    "#   classA/\n",
    "#   classB/\n",
    "#   classC/\n",
    "#   classD/\n",
    "\n",
    "# ========== 3. 训练超参数 ==========\n",
    "batch_size   = 32\n",
    "num_workers  = 0\n",
    "num_epochs   = 30\n",
    "lr           = 1e-3      # 只训练线性头，可以稍微大一点\n",
    "weight_decay = 1e-2\n",
    "\n",
    "# ========== 4. 随机种子（保证每次划分一致） ==========\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "local_model_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/medsiglip-448\"  # TODO: 改成你的路径\n",
    "\n",
    "train_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/data_split/train\"\n",
    "val_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/data_split/val\"\n",
    "image_size = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 从本地加载 MedSigLIP，多模态模型中抽出视觉塔\n",
    "print(\"Loading base model from local dir:\", local_model_dir)\n",
    "\n",
    "raw_model = AutoModel.from_pretrained(\n",
    "    local_model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "print(\"raw_model class:\", type(raw_model))\n",
    "\n",
    "# ---- 关键：只抽出“视觉 encoder” ----\n",
    "if hasattr(raw_model, \"vision_model\"):\n",
    "    # 大多数 CLIP/SigLIP 多模态模型的视觉塔都叫 vision_model\n",
    "    img_encoder = raw_model.vision_model\n",
    "    print(\"Use raw_model.vision_model as image encoder.\")\n",
    "elif hasattr(raw_model, \"get_image_features\"):\n",
    "    # 有些实现是直接在 model 上提供 get_image_features\n",
    "    img_encoder = raw_model\n",
    "    print(\"Use raw_model itself as image encoder (get_image_features).\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"在 raw_model 里找不到 vision_model 或 get_image_features，\"\n",
    "        \"请 print(raw_model) 看看结构，然后再定位视觉塔。\"\n",
    "    )\n",
    "\n",
    "img_encoder.to(device)\n",
    "img_encoder.eval()\n",
    "\n",
    "# ---- 用 dummy 探测 embedding 维度 ----\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 3, image_size, image_size).to(device)   # [1,3,448,448]\n",
    "\n",
    "    try:\n",
    "        out = img_encoder(pixel_values=dummy)   # 优先用 keyword\n",
    "    except TypeError:\n",
    "        out = img_encoder(dummy)               # 有的模型只收 positional\n",
    "\n",
    "    if hasattr(out, \"image_embeds\"):\n",
    "        feats = out.image_embeds                    # [1, D]\n",
    "    elif hasattr(out, \"pooler_output\"):\n",
    "        feats = out.pooler_output                   # [1, D]\n",
    "    elif hasattr(out, \"last_hidden_state\"):\n",
    "        feats = out.last_hidden_state.mean(dim=1)   # [1, D]\n",
    "    elif isinstance(out, torch.Tensor):\n",
    "        feats = out\n",
    "    else:\n",
    "        print(\"Unknown output type:\", type(out))\n",
    "        print(out)\n",
    "        raise RuntimeError(\n",
    "            \"无法从 img_encoder 的输出中找到特征，请 print(out) 再调整逻辑。\"\n",
    "        )\n",
    "\n",
    "embed_dim = feats.shape[-1]\n",
    "print(\"image embed dim:\", embed_dim)\n",
    "\n",
    "\n",
    "# ---- 封装分类模型：视觉塔 + 线性 head ----\n",
    "class MedSigVisionClassifier(nn.Module):\n",
    "    def __init__(self, img_encoder, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = img_encoder\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # pixel_values: [B,3,448,448]\n",
    "        try:\n",
    "            out = self.encoder(pixel_values=pixel_values)\n",
    "        except TypeError:\n",
    "            out = self.encoder(pixel_values)\n",
    "\n",
    "        if hasattr(out, \"image_embeds\"):\n",
    "            feats = out.image_embeds\n",
    "        elif hasattr(out, \"pooler_output\"):\n",
    "            feats = out.pooler_output\n",
    "        elif hasattr(out, \"last_hidden_state\"):\n",
    "            feats = out.last_hidden_state.mean(dim=1)\n",
    "        elif isinstance(out, torch.Tensor):\n",
    "            feats = out\n",
    "        else:\n",
    "            raise RuntimeError(\"encoder 输出里找不到特征，需根据实际结构单独处理。\")\n",
    "\n",
    "        # L2 归一化（保持和 CLIP 系一致的风格）\n",
    "        feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        logits = self.head(feats)    # [B,num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ==== 构建模型 + DataParallel ====\n",
    "model = MedSigVisionClassifier(img_encoder, embed_dim, 4)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"使用\", torch.cuda.device_count(), \"张 GPU 进行 DataParallel\")\n",
    "    model = nn.DataParallel(model)   # 在多卡上自动切 batch\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 关键：取出真正的模型（DataParallel 包了一层壳）\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# 先冻结视觉塔，只训 head 当 baseline\n",
    "for p in core.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in core.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# 统计参数量用 core（真正的模型）\n",
    "total_params = sum(p.numel() for p in core.parameters())\n",
    "trainable_params = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "print(f\"总参数量: {total_params:,}\")\n",
    "print(f\"当前可训练参数量(仅 head): {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% DataLoader：自己写 448x448 的 transform\n",
    "# 灰度医学片 -> 3 通道，Resize 到 448，归一化\n",
    "\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std  = [0.5, 0.5, 0.5]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"train samples:\", len(train_dataset))\n",
    "print(\"val samples:\", len(val_dataset))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d204a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 版 Linear：在 frozen 的 base Linear 外面加一个低秩补丁 ΔW = BA\n",
    "import math\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, r=8, alpha=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert isinstance(base_linear, nn.Linear)\n",
    "        self.in_features  = base_linear.in_features\n",
    "        self.out_features = base_linear.out_features\n",
    "\n",
    "        self.base = base_linear\n",
    "        self.base.weight.requires_grad = False\n",
    "        if self.base.bias is not None:\n",
    "            self.base.bias.requires_grad = False\n",
    "\n",
    "        self.r = r\n",
    "        self.lora_A = nn.Linear(self.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, self.out_features, bias=False)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # ⭐ 关键：LoRA 权重搬到和 base 同一个 device\n",
    "        device = self.base.weight.device\n",
    "        self.lora_A.to(device)\n",
    "        self.lora_B.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        lora_out = self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        return base_out + lora_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def add_lora_fc_last_blocks(model, r=8, alpha=16, dropout=0.1, last_n_blocks=4):\n",
    "    \"\"\"\n",
    "    只在视觉编码器最后 last_n_blocks 个 block 的 mlp.fc1 / mlp.fc2 上打 LoRA，\n",
    "    其他层全部冻结；head + LoRA 参与训练。\n",
    "    \"\"\"\n",
    "    # 兼容 DataParallel / DDP\n",
    "    core = model.module if hasattr(model, \"module\") else model\n",
    "\n",
    "    # 你的视觉塔是 SiglipVisionTransformer\n",
    "    vit = core.encoder\n",
    "\n",
    "    # 1. 拿到所有 block（每个 block 是一个 Transformer 层）\n",
    "    try:\n",
    "        blocks = list(vit.encoder.layers)  # ✅ SiglipVisionTransformer 里是 encoder.layers\n",
    "    except AttributeError:\n",
    "        print(\"encoder 结构如下，请发我这个打印：\")\n",
    "        print(vit)\n",
    "        raise\n",
    "\n",
    "    n_blocks = len(blocks)\n",
    "    if last_n_blocks > n_blocks:\n",
    "        last_n_blocks = n_blocks\n",
    "        print(f\"⚠ last_n_blocks 超过总层数，自动改为 {n_blocks}\")\n",
    "\n",
    "    target_blocks = blocks[-last_n_blocks:]\n",
    "\n",
    "    # 2. 先把整个 encoder 全部 requires_grad=False （base 权重全部冻结）\n",
    "    for p in vit.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # 3. 在目标 block 的 mlp.fc1 / mlp.fc2 注入 LoRA\n",
    "    for idx, block in enumerate(target_blocks):\n",
    "        # SigLIP 的每层里一般有 block.mlp.fc1 / block.mlp.fc2\n",
    "        mlp = block.mlp\n",
    "\n",
    "        # 用你自己定义好的 LoRALinear 包一下原始 Linear\n",
    "        mlp.fc1 = LoRALinear(mlp.fc1, r=r, alpha=alpha, dropout=dropout)\n",
    "        mlp.fc2 = LoRALinear(mlp.fc2, r=r, alpha=alpha, dropout=dropout)\n",
    "\n",
    "        real_idx = n_blocks - last_n_blocks + idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9038be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只优化 LoRA + head\n",
    "add_lora_fc_last_blocks(model, r=8, alpha=16, dropout=0.1, last_n_blocks=4)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "print(\"可训练参数个数:\", sum(p.numel() for p in trainable_params))\n",
    "\n",
    "# %% 训练 & 验证函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,\n",
    ")\n",
    "\n",
    "def train_one_epoch(epoch: int):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs   = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\":  f\"{correct/total:.4f}\",\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(epoch: int):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} [val]  \")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs   = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\":  f\"{correct/total:.4f}\",\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93365f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 取出真正的模型（去掉 DataParallel 壳）\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "total_params = sum(p.numel() for p in core.parameters())\n",
    "trainable_params = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "\n",
    "# 统计 LoRA 专属参数（lora_A / lora_B）\n",
    "lora_params = 0\n",
    "for name, p in core.named_parameters():\n",
    "    if (\"lora_A\" in name) or (\"lora_B\" in name):\n",
    "        lora_params += p.numel()\n",
    "\n",
    "# 统计 head 参数（你分类头叫 core.head）\n",
    "head_params = sum(p.numel() for p in core.head.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"====== 参数统计（LoRA 注入后）======\")\n",
    "print(f\"总参数量          : {total_params:,}\")\n",
    "print(f\"当前可训练参数量  : {trainable_params:,}\")\n",
    "print(f\"  其中 LoRA 参数  : {lora_params:,}\")\n",
    "print(f\"  其中 head 参数  : {head_params:,}\")\n",
    "print(f\"可训练比例        : {trainable_params / total_params:.4%}\")\n",
    "print(f\"LoRA 占总参数比例 : {lora_params / total_params:.4%}\")\n",
    "print(f\"LoRA 占可训练比例 : {lora_params / trainable_params:.4%}\")\n",
    "print(\"✅ 当前参与训练的参数：\")\n",
    "\n",
    "for name, p in core.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"Train:\",name)\n",
    "    else :\n",
    "        print(\"Freez:\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cffb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "def train_one_epoch_amp(epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # ⭐ 用 tqdm 包一层 dataloader\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\", leave=False)\n",
    "\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # ⭐ 动态在进度条上显示当前 loss / acc\n",
    "        cur_loss = running_loss / total\n",
    "        cur_acc  = correct / total\n",
    "        pbar.set_postfix(loss=f\"{cur_loss:.4f}\", acc=f\"{cur_acc:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"[Epoch {epoch}] train_loss={epoch_loss:.4f} train_acc={epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch_amp(epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} [val]  \", leave=False)\n",
    "\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        cur_loss = running_loss / total\n",
    "        cur_acc  = correct / total\n",
    "        pbar.set_postfix(loss=f\"{cur_loss:.4f}\", acc=f\"{cur_acc:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"[Epoch {epoch}] val_loss={epoch_loss:.4f} val_acc={epoch_acc:.4f}\")\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ada64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 主训练循环\n",
    "log_path = \"trainlog_linear.log\"\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch_amp(epoch)\n",
    "    val_loss, val_acc     = eval_one_epoch_amp(epoch)\n",
    "    line = (\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(line)\n",
    "\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"Best val_acc:\", best_val_acc)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    save_path = f\"./medsiglip448_cls_best_acc{best_val_acc:.4f}.pth\"\n",
    "    torch.save(best_state_dict, save_path)\n",
    "    print(\"Saved best model to:\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
