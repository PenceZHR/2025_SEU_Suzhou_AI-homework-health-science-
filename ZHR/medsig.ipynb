{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6018ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import timm\n",
    "import shutil\n",
    "\n",
    "# ========== 1. 设备 ==========\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ========== 2. 数据路径 ==========\n",
    "# ❗❗❗ 把这个改成你“四个类别”所在的文件夹 ❗❗❗\n",
    "root_dir = r\"D:/OneDriveFiles/OneDrive/人工智能基础期末/dataset2/\"\n",
    "\n",
    "# 目录结构要求：\n",
    "# root_dir/\n",
    "#   classA/\n",
    "#   classB/\n",
    "#   classC/\n",
    "#   classD/\n",
    "\n",
    "# ========== 3. 训练超参数 ==========\n",
    "batch_size   = 64\n",
    "num_workers  = 0\n",
    "num_epochs   = 30\n",
    "lr           = 1e-3      # 只训练线性头，可以稍微大一点\n",
    "weight_decay = 1e-2\n",
    "\n",
    "# ========== 4. 随机种子（保证每次划分一致） ==========\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "local_model_dir = r\"/root/人工智能基础期末作业/medsig_model\"  # TODO: 改成你的路径\n",
    "\n",
    "train_dir = r\"/root/人工智能基础期末作业/data_split/train\"\n",
    "val_dir = r\"/root/人工智能基础期末作业/data_split/val\"\n",
    "image_size = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30cb06ff-3143-43b0-9ed8-222b284fb2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class 0', 'class 1', 'class 2', 'class 3', '.DS_Store']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DIR = Path(\"/root/人工智能基础期末作业/dataset2/\")\n",
    "OUT_DIR = Path(\"/root/人工智能基础期末作业/data_split\")    # 拆分后的 train/val 会放在这里\n",
    "\n",
    "classes = os.listdir(RAW_DIR)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fffe203e-3940-4937-b990-d247cfbc58b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现的类别： ['class 0', 'class 1', 'class 2', 'class 3']\n"
     ]
    }
   ],
   "source": [
    "classes = [d for d in os.listdir(RAW_DIR) if (RAW_DIR / d).is_dir()]\n",
    "print(\"发现的类别：\", classes)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for cls in classes:\n",
    "        (OUT_DIR / phase / cls).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75718a6b-6cdd-4579-ac19-12efac4f9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.8   # 训练集占 80%\n",
    "# val_ratio   = 0.2   # 验证集占 20%，train_ratio + val_ratio 应该 = 1\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"使用设备:\", device)\n",
    "\n",
    "\n",
    "# for cls in classes:\n",
    "#     src_dir = RAW_DIR / cls\n",
    "#     files = [f for f in os.listdir(src_dir)\n",
    "#              if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "#     random.shuffle(files)\n",
    "#     n = len(files)\n",
    "#     n_train = int(n * train_ratio)\n",
    "#     # val 集就是剩下的\n",
    "#     train_files = files[:n_train]\n",
    "#     val_files   = files[n_train:]\n",
    "\n",
    "#     print(f\"{cls}: 总数 {n}, 训练 {len(train_files)}, 验证 {len(val_files)}\")\n",
    "\n",
    "#     # 复制到目标文件夹（想省空间可以用 shutil.move）\n",
    "#     for fname in train_files:\n",
    "#         shutil.copy(src_dir / fname, OUT_DIR / \"train\" / cls / fname)\n",
    "#     for fname in val_files:\n",
    "#         shutil.copy(src_dir / fname, OUT_DIR / \"val\" / cls / fname)\n",
    "\n",
    "# print(\"划分完成，已保存到\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e04495e-54e2-48f9-9e89-14281df21e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from local dir: /root/人工智能基础期末作业/medsig_model\n",
      "raw_model class: <class 'transformers.models.siglip.modeling_siglip.SiglipModel'>\n",
      "Use raw_model.vision_model as image encoder.\n",
      "image embed dim: 1152\n",
      "使用 2 张 GPU 进行 DataParallel\n",
      "总参数量: 428,570,052\n",
      "当前可训练参数量(仅 head): 4,612\n"
     ]
    }
   ],
   "source": [
    "# %% 从本地加载 MedSigLIP，多模态模型中抽出视觉塔\n",
    "print(\"Loading base model from local dir:\", local_model_dir)\n",
    "\n",
    "raw_model = AutoModel.from_pretrained(\n",
    "    local_model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "print(\"raw_model class:\", type(raw_model))\n",
    "\n",
    "# ---- 关键：只抽出“视觉 encoder” ----\n",
    "if hasattr(raw_model, \"vision_model\"):\n",
    "    # 大多数 CLIP/SigLIP 多模态模型的视觉塔都叫 vision_model\n",
    "    img_encoder = raw_model.vision_model\n",
    "    print(\"Use raw_model.vision_model as image encoder.\")\n",
    "elif hasattr(raw_model, \"get_image_features\"):\n",
    "    # 有些实现是直接在 model 上提供 get_image_features\n",
    "    img_encoder = raw_model\n",
    "    print(\"Use raw_model itself as image encoder (get_image_features).\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"在 raw_model 里找不到 vision_model 或 get_image_features，\"\n",
    "        \"请 print(raw_model) 看看结构，然后再定位视觉塔。\"\n",
    "    )\n",
    "\n",
    "img_encoder.to(device)\n",
    "img_encoder.eval()\n",
    "\n",
    "# ---- 用 dummy 探测 embedding 维度 ----\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 3, image_size, image_size).to(device)   # [1,3,448,448]\n",
    "\n",
    "    try:\n",
    "        out = img_encoder(pixel_values=dummy)   # 优先用 keyword\n",
    "    except TypeError:\n",
    "        out = img_encoder(dummy)               # 有的模型只收 positional\n",
    "\n",
    "    if hasattr(out, \"image_embeds\"):\n",
    "        feats = out.image_embeds                    # [1, D]\n",
    "    elif hasattr(out, \"pooler_output\"):\n",
    "        feats = out.pooler_output                   # [1, D]\n",
    "    elif hasattr(out, \"last_hidden_state\"):\n",
    "        feats = out.last_hidden_state.mean(dim=1)   # [1, D]\n",
    "    elif isinstance(out, torch.Tensor):\n",
    "        feats = out\n",
    "    else:\n",
    "        print(\"Unknown output type:\", type(out))\n",
    "        print(out)\n",
    "        raise RuntimeError(\n",
    "            \"无法从 img_encoder 的输出中找到特征，请 print(out) 再调整逻辑。\"\n",
    "        )\n",
    "\n",
    "embed_dim = feats.shape[-1]\n",
    "print(\"image embed dim:\", embed_dim)\n",
    "\n",
    "\n",
    "# ---- 封装分类模型：视觉塔 + 线性 head ----\n",
    "class MedSigVisionClassifier(nn.Module):\n",
    "    def __init__(self, img_encoder, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = img_encoder\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # pixel_values: [B,3,448,448]\n",
    "        try:\n",
    "            out = self.encoder(pixel_values=pixel_values)\n",
    "        except TypeError:\n",
    "            out = self.encoder(pixel_values)\n",
    "\n",
    "        if hasattr(out, \"image_embeds\"):\n",
    "            feats = out.image_embeds\n",
    "        elif hasattr(out, \"pooler_output\"):\n",
    "            feats = out.pooler_output\n",
    "        elif hasattr(out, \"last_hidden_state\"):\n",
    "            feats = out.last_hidden_state.mean(dim=1)\n",
    "        elif isinstance(out, torch.Tensor):\n",
    "            feats = out\n",
    "        else:\n",
    "            raise RuntimeError(\"encoder 输出里找不到特征，需根据实际结构单独处理。\")\n",
    "\n",
    "        # L2 归一化（保持和 CLIP 系一致的风格）\n",
    "        feats = feats / (feats.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "        logits = self.head(feats)    # [B,num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ==== 构建模型 + DataParallel ====\n",
    "model = MedSigVisionClassifier(img_encoder, embed_dim, 4)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"使用\", torch.cuda.device_count(), \"张 GPU 进行 DataParallel\")\n",
    "    model = nn.DataParallel(model)   # 在多卡上自动切 batch\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 关键：取出真正的模型（DataParallel 包了一层壳）\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# 先冻结视觉塔，只训 head 当 baseline\n",
    "for p in core.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in core.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# 统计参数量用 core（真正的模型）\n",
    "total_params = sum(p.numel() for p in core.parameters())\n",
    "trainable_params = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "print(f\"总参数量: {total_params:,}\")\n",
    "print(f\"当前可训练参数量(仅 head): {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2319cc0e-b2e4-4da0-9aae-f1f826a06f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['class 0', 'class 1', 'class 2', 'class 3']\n",
      "train samples: 5841\n",
      "val samples: 1462\n"
     ]
    }
   ],
   "source": [
    "# %% DataLoader：自己写 448x448 的 transform\n",
    "# 灰度医学片 -> 3 通道，Resize 到 448，归一化\n",
    "\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std  = [0.5, 0.5, 0.5]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
    "val_dataset   = ImageFolder(val_dir,   transform=val_transform)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"train samples:\", len(train_dataset))\n",
    "print(\"val samples:\", len(val_dataset))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60070563-7824-4bec-b041-60a89183a4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 数量: 27\n",
      "总参数量: 428,570,052\n",
      "当前可训练参数量: 45,723,124\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ==== 先拿到真正的模型 ====\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "# ===== 设定要冻结的层数 =====\n",
    "K = 24  # 冻结前 K 层，训练第 K 层及之后的层（总共 27 层的话就是训最后 7 层）\n",
    "\n",
    "# 1. 先把整个 encoder 都冻住\n",
    "for p in core.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2. 拿到 block 列表（就是你之前打印的 encoder.layers 这个 ModuleList）\n",
    "blocks = core.encoder.encoder.layers   # 注意是 encoder.encoder.layers\n",
    "print(\"block 数量:\", len(blocks))      # 应该是 27\n",
    "\n",
    "# 3. 冻结前 K 层，解冻后面的\n",
    "for i, block in enumerate(blocks):\n",
    "    if i < K:\n",
    "        # 前 K 层保持冻结（其实已经全冻过了，这里写不写都行）\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = False\n",
    "    else:\n",
    "        # 只训练后面的 block（第 K 层及以后）\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# 4. head 一定要解冻\n",
    "for p in core.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# 5. 看一下现在的参数情况（用 core 统计真正模型）\n",
    "total_params = sum(p.numel() for p in core.parameters())\n",
    "trainable_params = sum(p.numel() for p in core.parameters() if p.requires_grad)\n",
    "print(f\"总参数量: {total_params:,}\")\n",
    "print(f\"当前可训练参数量: {trainable_params:,}\")\n",
    "\n",
    "# 6. 冻结/解冻设置好之后，再重建 optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],  # 这里用 model，兼容 DataParallel\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d81cc4a-a938-4212-9c81-74a07abce59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 训练 & 验证函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,\n",
    ")\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch: int):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs   = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\":  f\"{correct/total:.4f}\",\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(epoch: int):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(val_loader, desc=f\"Epoch {epoch} [val]  \")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs   = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{running_loss/total:.4f}\",\n",
    "            \"acc\":  f\"{correct/total:.4f}\",\n",
    "        })\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc  = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93801fda-6f42-4015-a3c4-16edeab52edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 主训练循环\n",
    "log_path = \"trainlog_23.log\"\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75341aa5-acc9-44bc-9313-6aba8c54ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71358ddc-31c0-4705-bdaf-fc253f6afba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366094041f124b09a1429bccb71174c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [train]:   0%|          | 0/92 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     val_loss, val_acc     = eval_one_epoch(epoch)\n\u001b[32m      4\u001b[39m     line = (\n\u001b[32m      5\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     28\u001b[39m logits = model(imgs)\n\u001b[32m     29\u001b[39m loss   = criterion(logits, labels)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m optimizer.step()\n\u001b[32m     34\u001b[39m running_loss += loss.item() * imgs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_acc     = eval_one_epoch(epoch)\n",
    "    line = (\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(line)\n",
    "\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"Best val_acc:\", best_val_acc)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    save_path = f\"./medsiglip448_cls_best_acc{best_val_acc:.4f}.pth\"\n",
    "    torch.save(best_state_dict, save_path)\n",
    "    print(\"Saved best model to:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a459a551-630f-4ae9-a9aa-5dfd4b8c0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "def eval_and_confmat(model, loader, device, class_names):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            y_true.append(y.cpu().numpy())\n",
    "            y_pred.append(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    # 1) 原始混淆矩阵（计数）\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix (counts)\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2) 归一化混淆矩阵（每行归一化，更好看“每类召回”）\n",
    "    cm_norm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))), normalize=\"true\")\n",
    "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_norm, display_labels=class_names)\n",
    "    disp2.plot(cmap=\"Blues\", values_format=\".2f\")\n",
    "    plt.title(\"Confusion Matrix (normalized by true class)\")\n",
    "    plt.show()\n",
    "\n",
    "    # 3) 每类 precision/recall/F1\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "    return cm, cm_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ac692-703d-4756-9db7-2a178966dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 DataParallel 外壳\n",
    "core = model.module if isinstance(model, nn.DataParallel) else model\n",
    "\n",
    "save_path = \"medsig_best_epoch_V2.pth\"   # 随便起个名字\n",
    "\n",
    "torch.save(core.state_dict(), save_path)\n",
    "print(\"已保存当前这一轮的参数到:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2825ec-efd3-4c58-968f-4db3960a2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续炼丹\n",
    "log_path = \"trainlog_23.log\"\n",
    "torch.cuda.empty_cache()\n",
    "epochs_append = 10\n",
    "for epoch in range(num_epochs, epochs_append + num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(epoch)\n",
    "    val_loss, val_acc     = eval_one_epoch(epoch)\n",
    "    line = (\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
    "        f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(line)\n",
    "\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line + \"\\n\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"Best val_acc:\", best_val_acc)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    save_path = f\"./medsiglip448_cls_best_acc{best_val_acc:.4f}.pth\"\n",
    "    torch.save(best_state_dict, save_path)\n",
    "    print(\"Saved best model to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b334dcf-3191-4017-9bbf-694964113720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. 先构建“同结构”的模型 ------------------------------------\n",
    "# 这里用你自己的模型类和参数\n",
    "# 比如你的是：\n",
    "# model_core = MedSigVisionClassifier(img_encoder, embed_dim, num_classes)\n",
    "torch.cuda.empty_cache()\n",
    "model_core = MedSigVisionClassifier(img_encoder, embed_dim, 4)  # 按你之前的一样写\n",
    "model_core = model_core.to(device)\n",
    "\n",
    "# 2. 读取 pth（state_dict） ------------------------------------\n",
    "state_dict_path = \"medsiglip448_cls_best_acc0.9770.pth\"   # 改成你自己的文件名\n",
    "state_dict = torch.load(state_dict_path, map_location=device)\n",
    "\n",
    "# 3. 把参数加载进模型 ------------------------------------------\n",
    "model_core.load_state_dict(state_dict)\n",
    "\n",
    "# 4. 如果你想多卡跑，可以再包一层 DataParallel（可选）\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model_core)\n",
    "else:\n",
    "    model = model_core\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ 模型加载完毕，可以直接用 model 做推理/验证\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dff7b15-d680-4108-be77-a90ee6b03d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.9762084592145015\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def eval_once(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    print(\"acc =\", correct / total)\n",
    "\n",
    "# 比如在 val_loader / test_loader 上跑一下\n",
    "eval_once(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a84e7-3336-47d9-a57a-4d9df87185ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
